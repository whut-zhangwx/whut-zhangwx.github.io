<!DOCTYPE html><html class="hide-aside" lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>Google Colab | 文羊羽</title><meta name="author" content="文羊羽"><meta name="copyright" content="文羊羽"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Google Colab google colab Quick Primer on Colab Jupyter Start Firstly, you need to have a google drive. (Of course you also have to get a google account before this.) New a folder as your work space i">
<meta property="og:type" content="article">
<meta property="og:title" content="Google Colab">
<meta property="og:url" content="https://whut-zhangwx.github.io/google-colab/index.html">
<meta property="og:site_name" content="文羊羽">
<meta property="og:description" content="Google Colab google colab Quick Primer on Colab Jupyter Start Firstly, you need to have a google drive. (Of course you also have to get a google account before this.) New a folder as your work space i">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://whut-zhangwx.github.io/white">
<meta property="article:published_time" content="2023-07-28T16:00:00.000Z">
<meta property="article:modified_time" content="2024-12-20T03:14:48.452Z">
<meta property="article:author" content="文羊羽">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://whut-zhangwx.github.io/white"><link rel="shortcut icon" href="/img/avatar.jpg"><link rel="canonical" href="https://whut-zhangwx.github.io/google-colab/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Google Colab',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: false,
  postUpdate: '2024-12-20 11:14:48'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"><link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">95</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">118</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">37</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">文羊羽</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout hide-aside" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">Google Colab</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-07-28T16:00:00.000Z" title="发表于 2023-07-29 00:00:00">2023-07-29</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-12-20T03:14:48.452Z" title="更新于 2024-12-20 11:14:48">2024-12-20</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">3.1k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>19分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Google Colab"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="post-content" id="article-container"><h2 id="google-colab">Google Colab</h2>
<p><a target="_blank" rel="noopener" href="https://colab.google/">google colab</a><br>
<a target="_blank" rel="noopener" href="https://colab.research.google.com/github/google/picatrix/blob/main/notebooks/Quick_Primer_on_Colab_Jupyter.ipynb">Quick Primer on Colab Jupyter</a></p>
<h2 id="start">Start</h2>
<p>Firstly, you need to have a <a target="_blank" rel="noopener" href="https://drive.google.com/">google drive</a>. (Of course you also have to get a google account before this.)</p>
<p>New a folder as your work space in your google drive.</p>
<ul>
<li>
<p>If you already have a .ipynb file, you can double click it directly (or right click it &gt; Open with &gt; Google Colaboratory) to open this .ipynb file with Colab.</p>
</li>
<li>
<p>If you want to new a .ipynb file, just right click blank space &gt; More &gt; Google Colaboratory, then google drive will create a Untitled.ipynb file and open it with Colab automatically.</p>
</li>
</ul>
<p>The use of Colab is vary similar with jupyter notebook.</p>
<h2 id="some-skills">Some skills</h2>
<p>You can add <code>!</code> before a command to indicate it should be run in shell. For example</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">! pip list</span><br></pre></td></tr></table></figure>
<p>By runing this command, Colab will return the names of all the packages it has installed including most common packages.</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">matplotlib                       3.7.1</span><br><span class="line">numpy                            1.22.4</span><br><span class="line">pandas                           1.5.3</span><br><span class="line">Pillow                           9.4.0</span><br><span class="line">tensorflow                       2.12.0</span><br><span class="line">torch                            2.0.1+cu118</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>If you want to install another package, take the <code>lightning</code> for an example, use the command below.</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">! pip install lightning</span><br></pre></td></tr></table></figure>
<p>Once you finish the install, you can import it in your python code by add this command.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> lightning</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Note: Using short cut key <code>Ctrl + Enter</code> to run the code block quickly.</p>
</blockquote>
<h3 id="mount-to-google-drive">Mount to google drive</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> google.colab <span class="keyword">import</span> drive</span><br><span class="line">drive.mount(<span class="string">&#x27;/content/drive&#x27;</span>)</span><br><span class="line"><span class="comment"># Mounted at /content/drive</span></span><br></pre></td></tr></table></figure>
<h3 id="change-working-directory">Change working directory</h3>
<p>Using <code>os.getcwd()</code> or <code>!pwd</code> to get current working directory</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.getcwd()</span><br><span class="line"><span class="comment"># /content</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">!pwd</span><br><span class="line"><span class="comment"># /content</span></span><br></pre></td></tr></table></figure>
<p>Using <code>os.chdir('...')</code> or magic cd <code>%cd '...'</code> to change working directory to your folder of Google Driver.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">os.chdir(<span class="string">&#x27;/content/drive/MyDrive/Colab Notebooks&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%cd <span class="string">&#x27;/content/drive/MyDrive/Colab Notebooks&#x27;</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>Note: Note that <code>!cd '...'</code> doesn’t work for this purpose because it will take effect only in the current code block.</p>
</blockquote>
<p><a target="_blank" rel="noopener" href="https://ipython.readthedocs.io/en/stable/interactive/magics.html#magic-cd">magic-cd | Ipython</a><br>
<a target="_blank" rel="noopener" href="https://www.runoob.com/linux/linux-comm-pwd.html">Linux pwd命令 | Runoob</a><br>
<a target="_blank" rel="noopener" href="https://www.runoob.com/linux/linux-comm-cd.html">Linux cd 命令 | Runoob</a></p>
<h3 id="how-to-change-python-version-in-google-colab">How to Change Python Version in Google Colab</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">! cat /etc/os-release <span class="comment"># check the system version of Colab</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">! python --version <span class="comment"># check the default version of Python in Colab</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">! ls /usr/<span class="built_in">bin</span>/python* <span class="comment"># check the availabel versions of python in Colab(Ubuntu)</span></span><br></pre></td></tr></table></figure>
<p><a target="_blank" rel="noopener" href="https://stackoverflow.com/a/68530310">How to Change Python Version in Google Colab</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">!cat /etc/os-release</span><br><span class="line"><span class="comment"># PRETTY_NAME=&quot;Ubuntu 22.04.2 LTS&quot;</span></span><br><span class="line"><span class="comment"># ...</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Check the default python version of colab</span></span><br><span class="line">!python --version</span><br><span class="line"><span class="comment"># Python 3.10.12</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Check the python version you have</span></span><br><span class="line">!ls /usr/<span class="built_in">bin</span>/python*</span><br><span class="line"><span class="comment"># /usr/bin/python3  /usr/bin/python3.10-config  /usr/bin/python3-config</span></span><br><span class="line"><span class="comment"># /usr/bin/python3.10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Install the python version you wish to list (python3.8 in my case)</span></span><br><span class="line">!sudo apt-get update -y</span><br><span class="line">!sudo apt-get install python3<span class="number">.8</span></span><br><span class="line"><span class="comment"># ...</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Check the availabel versions of python again</span></span><br><span class="line">! ls /usr/<span class="built_in">bin</span>/python*</span><br><span class="line"><span class="comment"># /usr/bin/python3     /usr/bin/python3.10-config  /usr/bin/python3-config</span></span><br><span class="line"><span class="comment"># /usr/bin/python3.10  /usr/bin/python3.8</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Switch source for python3 command</span></span><br><span class="line">!sudo update-alternatives --install /usr/<span class="built_in">bin</span>/python3 python3 /usr/<span class="built_in">bin</span>/python3<span class="number">.8</span> <span class="number">1</span></span><br><span class="line"><span class="comment"># update-alternatives: using /usr/bin/python3.8 to provide /usr/bin/python3 (python3) in auto mode</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Choose one of the given alternatives:</span></span><br><span class="line">!sudo update-alternatives --config python3</span><br><span class="line"><span class="comment"># There is only one alternative in link group python3 (providing /usr/bin/python3): /usr/bin/python3.8</span></span><br><span class="line"><span class="comment"># Nothing to configure.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Check the result</span></span><br><span class="line">!python3 --version</span><br><span class="line"><span class="comment"># Python 3.8.18</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Check the pip version</span></span><br><span class="line">!pip --version</span><br><span class="line"><span class="comment"># ...</span></span><br><span class="line"><span class="comment"># ModuleNotFoundError: No module named &#x27;pip&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Attention: Install pip (... needed!)</span></span><br><span class="line">!sudo apt install python3-pip</span><br><span class="line"><span class="comment"># ...</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># check pip version</span></span><br><span class="line">pip --version</span><br><span class="line"><span class="comment"># ...</span></span><br><span class="line"><span class="comment"># ModuleNotFoundError: No module named &#x27;distutils.cmd&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># install python3.8-distutils</span></span><br><span class="line">!sudo apt install python3<span class="number">.8</span>-distutils</span><br><span class="line"></span><br><span class="line"><span class="comment"># check pip version</span></span><br><span class="line">pip --version</span><br><span class="line"><span class="comment"># ...</span></span><br><span class="line"><span class="comment"># pip 22.0.2 from /usr/lib/python3/dist-packages/pip (python 3.8)</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>Alternative reading<br>
<a target="_blank" rel="noopener" href="https://tecadmin.net/linux-update-alternatives-command/">Update-alternatives Command: A Comprehensive Guide for Linux Users</a></p>
</blockquote>
  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!sudo update-alternatives: --install &lt;link&gt; &lt;name&gt; &lt;path&gt; &lt;priority&gt;</span><br></pre></td></tr></table></figure>
<h3 id="unzip">unzip</h3>
<p>Upload the .zip file to your google driver and open a .ipynb file in colab, then mount your google driver to colab and execute the following command.<br>
The first path is where your package is (<code>'.../Inception-v4.zip'</code> in my case), the second path is the destination folder.<br>
Note that the destination folder must be under <code>'/content/drive/MyDrive/'</code> which is your owe google driver directory.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!unzip <span class="string">&#x27;/content/drive/MyDrive/Colab Notebooks/Inception-v4.zip&#x27;</span> -d <span class="string">&#x27;/content/drive/MyDrive/Colab Notebooks&#x27;</span></span><br></pre></td></tr></table></figure>
<h3 id="how-to-find-the-location-of-package-that-you-pip-installed">How to find the location of package that you pip installed</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!python -m pip show &lt;package_name&gt;</span><br></pre></td></tr></table></figure>
<p>Take timm for example</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">!python -m pip show timm</span><br><span class="line"></span><br><span class="line">Name: timm</span><br><span class="line">Version: <span class="number">0.9</span><span class="number">.12</span></span><br><span class="line">Summary: PyTorch Image Models</span><br><span class="line">Home-page: https://github.com/huggingface/pytorch-image-models</span><br><span class="line">Author: Ross Wightman</span><br><span class="line">Author-email: ross@huggingface.co</span><br><span class="line">License: </span><br><span class="line">Location: /usr/local/lib/python3<span class="number">.10</span>/dist-packages</span><br><span class="line">Requires: huggingface-hub, pyyaml, safetensors, torch, torchvision</span><br><span class="line">Required-by: </span><br></pre></td></tr></table></figure>
<h2 id="reload-your-modified-python-file">Reload your modified python file</h2>
<p><a target="_blank" rel="noopener" href="https://saturncloud.io/blog/jupyter-notebook-reload-module-a-comprehensive-guide/">Jupyter Notebook Reload Module: A Comprehensive Guide</a></p>
<p>If you have imported a python file and later make changes to it, you’ll need to reload it in your Jupyter Notebook (or Colab) to take advantage of any recent changes.</p>
<p>Here’s the scenario. You are working in a Jupyter Notebook and you’ve imported a custom python file. While working in your notebook, you make some changes to the python file and want to work with those new changes in your Jupyter Notebook. After saving your python file, you run your import some_file code again. However, your recent changes don’t get imported.</p>
<p>You could restart your entire kernel, or you can simply reload the file by running this code:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> python_file</span><br><span class="line">...</span><br><span class="line"><span class="keyword">import</span> importlib</span><br><span class="line">importlib.reload(python_file)</span><br></pre></td></tr></table></figure>
<p>Once you run the code, you’ll see that any changes you made to your python file are correctly loaded into your jupyter notebook.</p>
<p>Using the %load_ext and %autoreload Magic Commands</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">%load_ext autoreload</span><br><span class="line">%autoreload <span class="number">2</span></span><br></pre></td></tr></table></figure>
<h3 id="typeerror">TypeError</h3>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">TypeError: reload() argument must be a module</span><br></pre></td></tr></table></figure>
<p>The reason why you got this error is that this module hadn’t been imported, or you used <code>from python_file import *</code> rather than <code>import python_file</code> so you didn’t import the python file (the module) actually.</p>
<p>There is two way to solve this problem.</p>
<p>The first is using <code>import python_file</code> to take place of <code>from python_file import *</code>;</p>
<p>The second is using <code>sys.modules['python_file']</code> to reload your module. Then reimport your python_file.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> python_file <span class="keyword">import</span> *</span><br><span class="line">...</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> importlib</span><br><span class="line">importlib.reload(sys.modules[<span class="string">&#x27;python_file&#x27;</span>])</span><br><span class="line"><span class="keyword">from</span> python_file <span class="keyword">import</span> *</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> python_file <span class="keyword">import</span> xxx</span><br><span class="line">...</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> importlib</span><br><span class="line">importlib.reload(sys.modules[<span class="string">&#x27;python_file&#x27;</span>])</span><br><span class="line"><span class="keyword">from</span> python_file <span class="keyword">import</span> xxx</span><br></pre></td></tr></table></figure>
<p>e.g.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> geoseg.datasets.potsdam_dataset <span class="keyword">import</span> PotsdamDataset</span><br><span class="line"></span><br><span class="line"><span class="comment"># modify PotsdamDataset</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> importlib</span><br><span class="line">importlib.reload(sys.modules[<span class="string">&#x27;geoseg.datasets.potsdam_dataset&#x27;</span>])</span><br><span class="line"><span class="keyword">from</span> geoseg.datasets.potsdam_dataset <span class="keyword">import</span> PotsdamDataset</span><br></pre></td></tr></table></figure>
<h2 id="cuda-in-colab">Cuda in Colab</h2>
<p>Click top left corner Edit &gt; Notebook settings &gt;</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Hardware accelerator: None/GPU/TPU</span><br><span class="line">GPU type: T4/A100/V100</span><br></pre></td></tr></table></figure>
<p>Use this command to test if GPU is available. You should get <code>True</code> if you set correctly.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">torch.cuda.is_available() <span class="comment"># True</span></span><br></pre></td></tr></table></figure>
<p>Get the number of GPU</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.cuda.device_count() <span class="comment"># 1</span></span><br></pre></td></tr></table></figure>
<p>Get the index of current device</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.cuda.current_device() <span class="comment"># 0</span></span><br></pre></td></tr></table></figure>
<p>Get the name of device</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.cuda.get_device_name(<span class="number">0</span>) <span class="comment"># Tesla T4</span></span><br></pre></td></tr></table></figure>
<p>Using this command to open <code>nvidia-smi</code> (Invidia System Management Interface) to check more information about your GPU.</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!/opt/bin/nvidia-smi</span><br></pre></td></tr></table></figure>
<p>By default, tensors are generated on the CPU. You have to manually make sure that the operation is done through the GPU.<br>
Using <code>.cuda()</code> you can convert (copy) Tensor from CPU to GPU.<br>
If there are multiple Gpus, use <code>.cuda(i)</code> to represent the i-th GPU, and i defaults to 0.<br>
Besides, you can use <code>.is_cuda</code> or <code>.is_cpu</code> to check whether a tensor is on GPU or CPU.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">x.is_cuda <span class="comment"># False</span></span><br><span class="line">x.is_cpu  <span class="comment"># True</span></span><br><span class="line">x = x.cuda(<span class="number">0</span>)</span><br><span class="line">x.is_cuda <span class="comment"># True</span></span><br><span class="line">x.is_cpu  <span class="comment"># False</span></span><br></pre></td></tr></table></figure>
<p>Using <code>.cpu()</code> you can convert(copy) a Tensor back to CPU from GPU. And you can print a Tensor’s device attribute to check it is on a CPU or a GPU.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="built_in">print</span>(x.device) <span class="comment"># cpu</span></span><br><span class="line">x = x.cuda(<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(x.device) <span class="comment"># cuda:0</span></span><br><span class="line">x = x.cpu()</span><br><span class="line"><span class="built_in">print</span>(x.device) <span class="comment"># cpu</span></span><br></pre></td></tr></table></figure>
<p>You can specify a tensor’s device attribute when you create it.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], device=<span class="string">&quot;cuda:0&quot;</span>)</span><br><span class="line">x.is_cuda <span class="comment"># True</span></span><br></pre></td></tr></table></figure>
<p><code>device</code> and <code>.to(device)</code> is commonly used in training with GPU. And This method is more recommended.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(device, <span class="built_in">type</span>(device)) <span class="comment"># cuda:0 &lt;class &#x27;torch.device&#x27;&gt;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], device=device)</span><br><span class="line"><span class="keyword">or</span></span><br><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]).to(device)</span><br><span class="line">x.is_cuda <span class="comment"># True</span></span><br></pre></td></tr></table></figure>
<p>If a calculation of a Tensor on a GPU device is performed, the result of the calculation will also be stored in that device:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]).cuda()</span><br><span class="line"><span class="built_in">print</span>(x.device) <span class="comment"># cuda:0</span></span><br><span class="line">y = x**<span class="number">2</span></span><br><span class="line"><span class="built_in">print</span>(y.device) <span class="comment"># cuda:0</span></span><br></pre></td></tr></table></figure>
<p>However, Tensor on different devices cannot be calculated directly, including Tensor on the GPU cannot be calculated directly with Tensor on the CPU, and  Tensor on different GPU devices cannot also be calculated directly:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]).cpu()</span><br><span class="line">y = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]).cuda()</span><br><span class="line"><span class="built_in">print</span>(x.device) <span class="comment"># cpu</span></span><br><span class="line"><span class="built_in">print</span>(y.device) <span class="comment"># cuda:0</span></span><br><span class="line">z = x + y</span><br><span class="line"></span><br><span class="line"><span class="comment"># RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!</span></span><br></pre></td></tr></table></figure>
<p>By default, your model is generated on CPU</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">model = torch.nn.Linear(<span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():</span><br><span class="line">  <span class="built_in">print</span>(param.device)  <span class="comment"># cpu</span></span><br><span class="line">  <span class="built_in">print</span>(param.is_cpu)  <span class="comment"># True</span></span><br><span class="line">  <span class="built_in">print</span>(param.is_cuda) <span class="comment"># False</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():</span><br><span class="line">  <span class="built_in">print</span>(param, param.size())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Parameter containing:</span></span><br><span class="line"><span class="comment"># tensor([[ 0.1516, -0.0096,  0.1179]], requires_grad=True) torch.Size([1, 3])</span></span><br><span class="line"><span class="comment"># Parameter containing:</span></span><br><span class="line"><span class="comment"># tensor([0.2088], requires_grad=True) torch.Size([1])</span></span><br></pre></td></tr></table></figure>
<p>You can convert your model to a GPU via <code>.cuda()</code> or <code>.to(device)</code> command. What <code>.cuda()</code> or <code>.to(device)</code> do is moving all model parameters and buffers to the GPU.<br>
<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=cuda#torch.nn.Module.cuda">torch.nn.Module.cuda</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">model = torch.nn.Linear(<span class="number">3</span>, <span class="number">1</span>).cuda(<span class="number">0</span>)</span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():</span><br><span class="line">  <span class="built_in">print</span>(param.device)  <span class="comment"># cduda:0</span></span><br><span class="line">  <span class="built_in">print</span>(param.is_cpu)  <span class="comment"># False</span></span><br><span class="line">  <span class="built_in">print</span>(param.is_cuda) <span class="comment"># True</span></span><br></pre></td></tr></table></figure>
<p>How to run your .py file rather than .ipynb on Colab</p>
<p>First change directory to where your .py file on your google driver</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">path=<span class="string">&quot;/content/drive/My Drive/...&quot;</span></span><br><span class="line">os.chdir(path)</span><br><span class="line">os.listdir(path)</span><br></pre></td></tr></table></figure>
<p>Then use the shell command to run your .py file.</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!python filename.py</span><br></pre></td></tr></table></figure>
<h3 id="empty-cache">empty_cache</h3>
<p>torch.cuda.empty_cache()</p>
<h2 id="pandas">pandas</h2>
<p><a target="_blank" rel="noopener" href="https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html">pandas.DataFrame</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">pd.DataFrame()</span><br></pre></td></tr></table></figure>
<h2 id="sklearn">sklearn</h2>
<p><a target="_blank" rel="noopener" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html">sklearn.model_selection.train_test_split</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">train_test_split()</span><br></pre></td></tr></table></figure>
<h2 id="segmentation-models-pytorch">segmentation_models_pytorch</h2>
<p><a target="_blank" rel="noopener" href="https://github.com/qubvel/segmentation_models.pytorch">segmentation_models.pytorch | GitHub</a><br>
<a target="_blank" rel="noopener" href="https://segmentation-modelspytorch.readthedocs.io/en/latest/">segmentation_models_pytorch’s documentation</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> segmentation_models_pytorch <span class="keyword">as</span> smp</span><br><span class="line"></span><br><span class="line">model = smp.Unet(</span><br><span class="line">  encoder_name = <span class="string">&#x27;inceptionv4&#x27;</span>, <span class="comment"># choose &#x27;inceptionv4&#x27; as encoder</span></span><br><span class="line">  encoder_weights = <span class="string">&#x27;imagenet&#x27;</span>, <span class="comment"># use `imagenet` pre-trained weights for encoder initialization</span></span><br><span class="line">  in_channels=<span class="number">3</span>, <span class="comment"># model input channels (1 for gray-scale images, 3 for RGB, default is 3)</span></span><br><span class="line">  classes=<span class="number">23</span>,    <span class="comment"># model output channels (number of classes in your dataset)</span></span><br><span class="line">  activation = <span class="literal">None</span>,</span><br><span class="line">  encoder_depth=<span class="number">5</span>,</span><br><span class="line">  decoder_channels=[<span class="number">256</span>, <span class="number">128</span>, <span class="number">64</span>, <span class="number">32</span>, <span class="number">16</span>]</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h3 id="smp-unet-http-error-502">smp.Unet() HTTP Error 502</h3>
<p>When you are invoking smp.Unet() or other model of smp, you get a http error as follows:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Downloading:</span><br><span class="line">&quot;http://data.lip6.fr/cadene/pretrainedmodels/inceptionv4-8e4777a0.pth&quot; </span><br><span class="line">to</span><br><span class="line">C:\Users\xiaophai/.cache\torch\hub\checkpoints\inceptionv4-8e4777a0.pth</span><br><span class="line"></span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  ...</span><br><span class="line">  File &quot;...\urllib\request.py&quot;, line 641, in http_error_default</span><br><span class="line">    raise HTTPError(req.full_url, code, msg, hdrs, fp)</span><br><span class="line">urllib.error.HTTPError: HTTP Error 502: Bad Gateway</span><br></pre></td></tr></table></figure>
<p>The reason for this is that the certificate of the website has expired, as shown in the following picture (the date is 2023-8-6).</p>
<p><img src="https://xiaophai-typora.oss-cn-shanghai.aliyuncs.com/20230806015601.png" alt="http://data.lip6.fr/cadene/pretrainedmodels/inceptionv4-8e4777a0.pth"></p>
<p>You need to download the .pth file manually (for example inceptionv4-8e4777a0.pth in my case) from the website, and move or copy it to where it should be (check the terminal to get the website and the path it should go).</p>
<h4 id="in-the-case-of-colab">In the case of Colab</h4>
<p>In the case of Colab, you first need to upload the .pth file to your Google Driver. Then use the following commands to copy the .pth file to the destination folder.</p>
<p>If you have not used torch hub before, you need to create a folder under this path (you can get it from the Error information) for torch hub first.</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!mkdir -p &#x27;/root/.cache/torch/hub/checkpoints/&#x27;</span><br></pre></td></tr></table></figure>
<p>Otherwise, you may get this error if you excute copy command directly.</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp: cannot create regular file &#x27;/root/.cache/torch/hub/checkpoints/&#x27;: No such file or directory</span><br></pre></td></tr></table></figure>
<p>Now you can use the following command to copy the .pth file to the destination folder. The first path is the path of your source file (…/inceptionv4-834777a0.pth in my case), and the second path is the path of your destination folder.</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">!cp -r</span><br><span class="line">&#x27;/content/drive/MyDrive/Colab Notebooks/Inception-v4/inceptionv4-8e4777a0.pth&#x27;</span><br><span class="line">&#x27;/root/.cache/torch/hub/checkpoints/&#x27;</span><br></pre></td></tr></table></figure>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://www.runoob.com/linux/linux-comm-mkdir.html">Linux mkdir 命令 | Runoob</a><br>
<a target="_blank" rel="noopener" href="https://www.runoob.com/linux/linux-comm-cp.html">Linux cp 命令 | Runoob</a></p>
</blockquote>
<h2 id="albumentations">albumentations</h2>
<p><a target="_blank" rel="noopener" href="https://albumentations.ai/docs/">Albumentations documentation</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> albumentations <span class="keyword">as</span> A</span><br><span class="line"></span><br><span class="line">t_train = A.Compose([</span><br><span class="line">  A.Resize(<span class="number">704</span>, <span class="number">1056</span>, interpolation=cv2.INTER_NEAREST),</span><br><span class="line">  A.HorizontalFlip(),</span><br><span class="line">  A.GridDistortion(p=<span class="number">0.2</span>)</span><br><span class="line">])</span><br><span class="line">t_val = A.Compose([</span><br><span class="line">  A.Resize(<span class="number">704</span>, <span class="number">1056</span>, interpolation=cv2.INTER_NEAREST),</span><br><span class="line">  A.HorizontalFlip(),</span><br><span class="line">  A.GridDistortion(p=<span class="number">0.2</span>)</span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<h3 id="torch-hub">torch.hub</h3>
<p><a target="_blank" rel="noopener" href="https://pytorch.org/hub/">Pytorch Hub</a><br>
<a target="_blank" rel="noopener" href="https://pytorch.org/hub/pytorch_vision_resnet/">pytorch_vision_resnet</a></p>
<h3 id="module-train-eval">Module.train&amp;eval</h3>
<p><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.train">torch.nn.Module.train</a></p>
<p><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.eval">torch.nn.Module.eval</a></p>
<h3 id="torch-save-load">torch.save&amp;load</h3>
<p><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.save.html">torch.save</a><br>
<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.load.html">torch.load</a><br>
<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/notes/serialization.html">Saving and loading pytorch tensors and module states</a></p>
<h4 id="saving-and-loading-tensors">saving and loading tensors</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">t1 = torch.tensor([<span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"><span class="comment"># saving t1 as tensor.pt</span></span><br><span class="line">torch.save(t1, <span class="string">&#x27;tensor.pt&#x27;</span>)</span><br><span class="line"><span class="comment"># loading tensor.pt to t2</span></span><br><span class="line">t2 = torch.load(<span class="string">&#x27;tensor.pt&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(t2)</span><br><span class="line"><span class="comment"># tensor([1, 2])</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>By convention, PyTorch files are typically written with a ‘.pt’ or ‘.pth’ extension.</p>
</blockquote>
<ul>
<li>you can also save multiple tensors as part of Python objects like tuples, lists, and dicts</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">t1 = torch.tensor([<span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line">t2 = torch.tensor([<span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># lists</span></span><br><span class="line"><span class="built_in">list</span> = [t1, t2]</span><br><span class="line">torch.save(<span class="built_in">list</span>, <span class="string">&#x27;tensor_list.pth&#x27;</span>)</span><br><span class="line"></span><br><span class="line">list_load = torch.load(<span class="string">&#x27;tensor_list.pth&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(list_load)</span><br><span class="line"><span class="comment"># [tensor([1, 2]), tensor([3, 4])]</span></span><br><span class="line"><span class="built_in">print</span>(list_load[<span class="number">0</span>]) <span class="comment"># tensor([1, 2])</span></span><br><span class="line"><span class="built_in">print</span>(list_load[<span class="number">1</span>]) <span class="comment"># tensor([3, 4])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># dicts</span></span><br><span class="line"><span class="built_in">dict</span> = &#123;<span class="string">&#x27;tensor1&#x27;</span>:t1, <span class="string">&#x27;tensor2&#x27;</span>:t2&#125;</span><br><span class="line">torch.save(<span class="built_in">dict</span>, <span class="string">&#x27;tensor_dict.pth&#x27;</span>)</span><br><span class="line"></span><br><span class="line">dict_load = torch.load(<span class="string">&#x27;tensor_dict.pth&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(dict_load)</span><br><span class="line"><span class="comment"># &#123;&#x27;tensor1&#x27;: tensor([1, 2]), &#x27;tensor2&#x27;: tensor([3, 4])&#125;</span></span><br><span class="line"><span class="built_in">print</span>(dict_load[<span class="string">&#x27;tensor1&#x27;</span>]) <span class="comment"># tensor([1, 2])</span></span><br><span class="line"><span class="built_in">print</span>(dict_load[<span class="string">&#x27;tensor2&#x27;</span>]) <span class="comment"># tensor([3, 4])</span></span><br></pre></td></tr></table></figure>
<ul>
<li>Saving and loading tensors preserves views</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">numbers = torch.arange(<span class="number">0</span>, <span class="number">10</span>)</span><br><span class="line">evens = numbers[<span class="number">0</span>:<span class="number">10</span>:<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># numbers and evens share the same “storage”</span></span><br><span class="line"><span class="built_in">print</span>(numbers.data_ptr()==evens.data_ptr()) <span class="comment"># True</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(numbers)</span><br><span class="line"><span class="comment"># tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])</span></span><br><span class="line"><span class="built_in">print</span>(evens)</span><br><span class="line"><span class="comment"># tensor([0, 2, 4, 6, 8])</span></span><br><span class="line">numbers *= <span class="number">2</span></span><br><span class="line"><span class="built_in">print</span>(numbers)</span><br><span class="line"><span class="comment"># tensor([ 0,  2,  4,  6,  8, 10, 12, 14, 16, 18])</span></span><br><span class="line"><span class="built_in">print</span>(evens)</span><br><span class="line"><span class="comment"># note that evens also changed</span></span><br><span class="line"><span class="comment"># tensor([ 0,  4,  8, 12, 16])</span></span><br><span class="line"></span><br><span class="line">torch.save([numbers, evens], <span class="string">&#x27;tensors.pt&#x27;</span>)</span><br><span class="line">loaded_numbers, loaded_evens = torch.load(<span class="string">&#x27;tensors.pt&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Saving and loading tensors preserves views</span></span><br><span class="line"><span class="comment"># numvers and evens still share the same “storage”</span></span><br><span class="line"><span class="built_in">print</span>(loaded_numbers.data_ptr()==loaded_evens.data_ptr()) <span class="comment"># True</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(loaded_numbers)</span><br><span class="line"><span class="comment"># tensor([ 0,  2,  4,  6,  8, 10, 12, 14, 16, 18])</span></span><br><span class="line"><span class="built_in">print</span>(loaded_evens)</span><br><span class="line"><span class="comment"># tensor([ 0,  4,  8, 12, 16])</span></span><br><span class="line">loaded_evens *= <span class="number">2</span></span><br><span class="line"><span class="built_in">print</span>(loaded_numbers)</span><br><span class="line"><span class="comment"># note that loaded_numbers also changed </span></span><br><span class="line"><span class="comment"># tensor([ 0,  2,  8,  6, 16, 10, 24, 14, 32, 18])</span></span><br><span class="line"><span class="built_in">print</span>(loaded_evens)</span><br><span class="line"><span class="comment"># tensor([ 0,  8, 16, 24, 32])</span></span><br></pre></td></tr></table></figure>
<ul>
<li>use <code>.clone()</code> to avoid unnecessary waste of storage</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">large = torch.arange(<span class="number">0</span>, <span class="number">1000</span>)</span><br><span class="line">small = large[<span class="number">0</span>:<span class="number">5</span>]</span><br><span class="line"><span class="built_in">print</span>(large.storage().size()) <span class="comment"># 1000</span></span><br><span class="line"><span class="built_in">print</span>(small.storage().size()) <span class="comment"># 1000</span></span><br><span class="line"></span><br><span class="line">small = large[<span class="number">0</span>:<span class="number">5</span>].clone()</span><br><span class="line"><span class="built_in">print</span>(small.storage().size()) <span class="comment"># 5</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">large = torch.arange(<span class="number">0</span>, <span class="number">1000</span>)</span><br><span class="line">small = large[<span class="number">0</span>:<span class="number">5</span>]</span><br><span class="line">torch.save(small.clone(), <span class="string">&#x27;small.pt&#x27;</span>)</span><br><span class="line">loaded_small = torch.load(<span class="string">&#x27;small.pt&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(loaded_small.storage().size()) <span class="comment"># 5</span></span><br></pre></td></tr></table></figure>
<h4 id="saving-and-loading-modules">Saving and loading Modules</h4>
<p><a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/beginner/saving_loading_models.html">Tutorial: Saving and loading modules</a></p>
<ul>
<li>saving and loading state_dict()</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyModule</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.layer1 = nn.Linear(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">        self.layer2 = nn.Linear(<span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = F.relu(self.layer1(x))</span><br><span class="line">        x = F.relu(self.layer2(x))</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line">model = MyModule()</span><br><span class="line"><span class="built_in">print</span>(model.state_dict())</span><br><span class="line"><span class="comment"># OrderedDict([</span></span><br><span class="line"><span class="comment"># (&#x27;layer1.weight&#x27;, tensor([[ 0.3531, -0.5274, -0.2317],</span></span><br><span class="line"><span class="comment">#                           [ 0.5642,  0.1380, -0.1637]])),</span></span><br><span class="line"><span class="comment"># (&#x27;layer1.bias&#x27;, tensor([-0.4267,  0.5400])),</span></span><br><span class="line"><span class="comment"># (&#x27;layer2.weight&#x27;, tensor([[-0.2539,  0.5889]])),</span></span><br><span class="line"><span class="comment"># (&#x27;layer2.bias&#x27;, tensor([0.6985]))</span></span><br><span class="line"><span class="comment"># ])</span></span><br><span class="line">torch.save(model.state_dict(), <span class="string">&#x27;mymodel_state_dict.pt&#x27;</span>)</span><br><span class="line">loaded_state_dict = torch.load(<span class="string">&#x27;mymodel_state_dict.pt&#x27;</span>)</span><br><span class="line"></span><br><span class="line">new_model = MyModule()</span><br><span class="line">new_model.load_state_dict(loaded_state_dict)</span><br></pre></td></tr></table></figure>
<ul>
<li>saving and loading entire model</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># save:</span></span><br><span class="line">torch.save(model, <span class="string">&#x27;mymodel.pt&#x27;</span>)</span><br><span class="line"><span class="comment"># load:</span></span><br><span class="line">model = torch.load(<span class="string">&#x27;mymodel.pt&#x27;</span>)</span><br><span class="line">model.<span class="built_in">eval</span>()</span><br></pre></td></tr></table></figure>
<h4 id="load-a-model-saved-from-a-cuda-device-to-a-cpu-device">Load a model saved from a CUDA device to a CPU device</h4>
<p>If you load a model saved from a CUDA device to a CPU device directly, you will get a RuntimeError as following:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">RuntimeError: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False.</span><br><span class="line">If you are running on a CPU-only machine, please use torch.load with map_location=torch.device(&#x27;cpu&#x27;) to map your storages to the CPU.</span><br></pre></td></tr></table></figure>
<p>You have to check <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.load.html">torch.load()</a> again to see its <code>map_location</code> attribute and add it to your <code>torch.load</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.load(<span class="string">&#x27;xxx.pt&#x27;</span>, map_location=torch.device(<span class="string">&#x27;cpu&#x27;</span>))</span><br></pre></td></tr></table></figure>
<h2 id="train-validation-and-test-sets">Train, Validation and Test Sets</h2>
<p><a target="_blank" rel="noopener" href="https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7">About Train, Validation and Test Sets in Machine Learning</a></p>
<h2 id="swin-transformer-for-classification">Swin-Transformer for Classification</h2>
<p>Data Set: flower-photos</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Folder Structure</span></span><br><span class="line">flower_photos  : <span class="number">3670</span></span><br><span class="line">├── daisy      : <span class="number">633</span></span><br><span class="line">├── dandelion  : <span class="number">898</span></span><br><span class="line">├── roses      : <span class="number">641</span></span><br><span class="line">├── sunflowers : <span class="number">699</span></span><br><span class="line">├── tulips     : <span class="number">799</span></span><br><span class="line">└── LICENSE.txt</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># read_split_data(root: str, val_rate: float = 0.2)</span></span><br><span class="line">classes    : num | calculation   | validation | train</span><br><span class="line">daisy      : <span class="number">633</span> | <span class="number">633</span>*<span class="number">0.2</span>=<span class="number">126.6</span> | <span class="number">126</span>        | <span class="number">507</span></span><br><span class="line">dandelion  : <span class="number">898</span> | <span class="number">898</span>*<span class="number">0.2</span>=<span class="number">179.6</span> | <span class="number">179</span>        | <span class="number">719</span></span><br><span class="line">roses      : <span class="number">641</span> | <span class="number">641</span>*<span class="number">0.2</span>=<span class="number">128.2</span> | <span class="number">128</span>        | <span class="number">513</span></span><br><span class="line">sunflowers : <span class="number">699</span> | <span class="number">699</span>*<span class="number">0.2</span>=<span class="number">139.8</span> | <span class="number">139</span>        | <span class="number">560</span></span><br><span class="line">tulips     : <span class="number">799</span> | <span class="number">799</span>*<span class="number">0.2</span>=<span class="number">159.8</span> | <span class="number">159</span>        | <span class="number">640</span></span><br><span class="line"><span class="comment"># ---------------------------------------------------</span></span><br><span class="line">total     : <span class="number">3670</span> |               | <span class="number">731</span>        | <span class="number">2939</span></span><br></pre></td></tr></table></figure>
<p><strong>Structure</strong><br>
Input Image: 3x224x224<br>
Number of Classes: 5<br>
Patch Partition: 56x56x48, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>224</mn><mn>4</mn></mfrac><mo>×</mo><mfrac><mn>224</mn><mn>4</mn></mfrac><mo>×</mo><mn>48</mn></mrow><annotation encoding="application/x-tex">\frac{224}{4}\times\frac{224}{4}\times 48</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1901em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8451em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">4</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">224</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.1901em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8451em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">4</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">224</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">48</span></span></span></span><br>
Stage 1</p>
<ul>
<li>Linear Embedding:</li>
<li>Swin Transformer Block:</li>
<li>Linear Embedding:</li>
<li>Swin Transformer Block:</li>
</ul>
<p>Stage 2</p>
<ul>
<li>Linear Embedding:</li>
<li>Swin Transformer Block:</li>
<li>Linear Embedding:</li>
<li>Swin Transformer Block:</li>
</ul>
<p>Stage 3</p>
<ul>
<li>Linear Embedding:</li>
<li>Swin Transformer Block:</li>
<li>Linear Embedding:</li>
<li>Swin Transformer Block:</li>
<li>Linear Embedding:</li>
<li>Swin Transformer Block:</li>
<li>Linear Embedding:</li>
<li>Swin Transformer Block:</li>
<li>Linear Embedding:</li>
<li>Swin Transformer Block:</li>
<li>Linear Embedding:</li>
<li>Swin Transformer Block:</li>
</ul>
<p>Stage 4</p>
<ul>
<li>Linear Embedding:</li>
<li>Swin Transformer Block:</li>
<li>Linear Embedding:</li>
<li>Swin Transformer Block:</li>
</ul>
<p>Stage 1: Linear Embedding + SwinTransformerBlock*2<br>
Stage 2: PatchMerging + SwinTransformerBlock*2<br>
Stage 3: PatchMerging + SwinTransformerBlock*6<br>
Stage 4: PatchMerging + SwinTransformerBlock*2</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://whut-zhangwx.github.io">文羊羽</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://whut-zhangwx.github.io/google-colab/">https://whut-zhangwx.github.io/google-colab/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://whut-zhangwx.github.io" target="_blank">文羊羽</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></div><div class="post_share"><div class="social-share" data-image="/white" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By 文羊羽</div><div class="footer_custom_text">Hi, welcome to my <a href="https://whut-zhangwx.github.io/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script><script>(() => {
  const $mermaidWrap = document.querySelectorAll('#article-container .mermaid-wrap')
  if ($mermaidWrap.length) {
    window.runMermaid = () => {
      window.loadMermaid = true
      const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'default' : 'default'

      Array.from($mermaidWrap).forEach((item, index) => {
        const mermaidSrc = item.firstElementChild
        const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
        const mermaidID = 'mermaid-' + index
        const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent
        mermaid.mermaidAPI.render(mermaidID, mermaidDefinition, (svgCode) => {
          mermaidSrc.insertAdjacentHTML('afterend', svgCode)
        })
      })
    }

    const loadMermaid = () => {
      window.loadMermaid ? runMermaid() : getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(runMermaid)
    }

    window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
  }
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>